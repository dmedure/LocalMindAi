[package]
name = "local-ai-agent"
version = "0.1.0"
description = "LocalMind AI Agent - Privacy-focused local AI with knowledge transfer"
authors = ["LocalMind Team"]
license = "MIT"
repository = "https://github.com/your-username/localmind"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[build-dependencies]
tauri-build = { version = "1.5", features = [] }

[dependencies]
# Core Tauri dependencies
tauri = { version = "1.5", features = ["api-all", "system-tray", "updater"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Async runtime
tokio = { version = "1.0", features = ["full"] }

# HTTP client for Ollama API
reqwest = { version = "0.11", features = ["json"] }

# UUID generation
uuid = { version = "1.0", features = ["v4", "serde"] }

# Date/time handling
chrono = { version = "0.4", features = ["serde"] }

# Error handling
anyhow = "1.0"

# File system utilities
dirs = "5.0"

# Configuration management
config = "0.13"

# Logging
tracing = "0.1"
tracing-subscriber = "0.3"

# System information
sysinfo = "0.29"

# File handling and compression (for knowledge transfer)
flate2 = "1.0"
zip = "0.6"

# Optional: Encryption for secure knowledge packages
aes-gcm = "0.10"
rand = "0.8"

# Optional: Database for local storage
rusqlite = { version = "0.29", features = ["bundled"] }

# Optional: ChromaDB client (if we implement direct integration)
# chroma-rs = "0.1"  # Note: This may not exist yet, we'll use HTTP client for now

# Optional: Local embedding models
# candle-core = "0.3"
# candle-nn = "0.3"
# candle-transformers = "0.3"

[features]
# This feature is used for production builds or when `devPath` points to the filesystem
# DO NOT REMOVE!!
custom-protocol = ["tauri/custom-protocol"]

# Optional features
default = []
encryption = ["aes-gcm", "rand"]
local-embeddings = [] # For future local embedding support
advanced-compression = ["flate2", "zip"]